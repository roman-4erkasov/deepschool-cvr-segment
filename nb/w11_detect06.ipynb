{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8e0de8-af1b-4610-b085-d422e6d23352",
   "metadata": {},
   "source": [
    "https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection\n",
    "\n",
    "https://www.kaggle.com/code/artgor/object-detection-with-pytorch-lightning\n",
    "\n",
    "https://gitee.com/wgs-gill/a-PyTorch-Tutorial-to-Object-Detection\n",
    "\n",
    "https://github.com/hse-ds/iad-deep-learning/blob/874790d122adce8f01fa207de8b36c37fbf56f53/2021/seminars/sem06/sem_06.ipynb#L742\n",
    "\n",
    "https://github.com/open-mmlab/mmdetection/blob/main/demo/MMDet_Tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f0d3b2-9248-4f50-bff9-d6eb3dda9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import math\n",
    "import torch\n",
    "import torch as th\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torchvision as thv\n",
    "# import torchmetrics as thm\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from src.datamodule import BarcodeDM\n",
    "from src.config import Config\n",
    "import albumentations as albu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3678c208-ae0b-40c9-a29c-5de16078bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MeanAveragePrecision()\n",
    "\n",
    "metric.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5e800f-d986-4c36-96b1-37b56faf6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.from_yaml(\"../config/baseline_detect.yml\")\n",
    "data = BarcodeDM(cfg.data_config, task=cfg.task, dry_run=True)\n",
    "\n",
    "data.prepare_data()\n",
    "data.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7046a5-c003-4ad4-8016-df1f83d99239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectModel(pl.LightningModule):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.model = thv.models.detection.fasterrcnn_resnet50_fpn(weights=\"COCO_V1\")\n",
    "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "        self.val_map = MeanAveragePrecision()\n",
    "        self.test_map = MeanAveragePrecision()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [p for p in model.parameters() if p.requires_grad], \n",
    "            lr=0.0001\n",
    "        )\n",
    "        return optomizer\n",
    "    # def configure_optimizers(self):\n",
    "    #     params = [p for p in model.parameters() if p.requires_grad]\n",
    "    #     optimizer = torch.optim.SGD(\n",
    "    #         params, lr=0.005, momentum=0.9, weight_decay=0.0005\n",
    "    #     )\n",
    "    #     scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    #         optimizer, step_size=3, gamma=0.1\n",
    "    #     )\n",
    "    #     return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\"}]\n",
    "            \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        images, targets = batch\n",
    "        loss_dict = self.model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        # return loss\n",
    "        print(f\"training_step:end:{loss=} {loss_dict=}\")\n",
    "        return {\n",
    "            'loss': loss, \n",
    "            'log': loss_dict, \n",
    "            'progress_bar': loss_dict\n",
    "        }\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        with th.no_grad():\n",
    "            pred = self.model(images)\n",
    "        self.val_map.update(preds=pred, target=targets\n",
    "        )\n",
    "        self.log_dict(self.val_map.compute(), on_step=False, on_epoch=True,prog_bar=False)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        with th.no_grad():\n",
    "            pred = self.model(images)\n",
    "        self.test_map.update(preds=pred, target=targets)\n",
    "        self.log_dict(self.test_map.compute(), on_step=True, on_epoch=True,prog_bar=False)\n",
    "\n",
    "    # def on_validation_epoch_start(self) -> None:\n",
    "    #     pass\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "         # self.log_dict(self._val_cls_metrics.compute(), on_epoch=True, on_step=False)\n",
    "         # self.log_dict(self._val_seg_metrics.compute(), on_epoch=True, on_step=False)\n",
    "         print(self.val_map.compute())\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        # self.log_dict(self._test_cls_metrics.compute(), on_epoch=True, on_step=False)\n",
    "        # self.log_dict(self._test_seg_metrics.compute(), on_epoch=True, on_step=False)\n",
    "        print(self.test_map.compute())\n",
    "\n",
    "    # def optimizer_step(self, *args, **kwargs):\n",
    "    #     super().optimizer_step(*args, **kwargs)\n",
    "    #     optimizer.step()\n",
    "    #     # self.lr_scheduler.step()  # Step per iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1181179-a67e-4179-afb7-268157264c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetectModel(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d341e94-404f-4a18-8445-a8a0cef1f880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data.train_dataloader()\n",
    "\n",
    "for b in train:\n",
    "    break\n",
    "\n",
    "b[0][0].min(), b[0][0].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c53b7d-d50b-4510-ac70-23aaa6809e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9c2c0-64af-41f4-9dd3-0c047707e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "images2, targets2 = model.model.transform(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f9e6cf-636a-4b67-b7f1-b899d623a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for target_idx, target in enumerate(targets):\n",
    "#     boxes = target[\"boxes\"]\n",
    "#     degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
    "#     if degenerate_boxes.any():\n",
    "#         # print the first degenerate box\n",
    "#         bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
    "#         degen_bb: List[float] = boxes[bb_idx].tolist()\n",
    "#         torch._assert(\n",
    "#             False,\n",
    "#             \"All bounding boxes should have positive height and width.\"\n",
    "#             f\" Found invalid box {degen_bb} for target at index {target_idx}.\",\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79687a-5995-4b63-9297-c31b034ece5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.model.backbone(images2.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76d155-6655-4caa-980e-56e699842cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.model.backbone.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0f5b8-4cf1-4bd9-a540-b28d90f9bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.isfinite(images2.tensors).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd965f-0c18-4c7d-829c-3d6e902ce4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.backbone.body.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d19b8-09ca-4247-bbcf-29c0ff06f9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.model.backbone.body[\"conv1\"](images2.tensors)\n",
    "x = model.model.backbone.body[\"bn1\"](x)\n",
    "x = model.model.backbone.body[\"relu\"](x)\n",
    "x = model.model.backbone.body[\"maxpool\"](x)\n",
    "\n",
    "x = model.model.backbone.body[\"layer1\"](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560cf35-67bb-44d1-b587-36a78bb50747",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.isfinite().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a208936-140d-4f72-8421-0c38448dc45f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict(model.model.backbone.body[\"layer2\"].named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6414dda-eba7-4498-b586-822aa135814d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "body_out = model.model.backbone.body(images2.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749665c2-c8eb-4591-b3d4-7c10cb848338",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_out[\"0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf786b4f-a4a1-4439-ab59-22ad585e4812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "body_out[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef286488-e036-40c3-98f3-bda10499ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals, proposal_losses = model.model.rpn(images2, features, targets2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3502e-fc76-4c28-98ac-9575315bce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97646404-2aad-478a-9263-6dce5b67c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc3a9a-a6dc-4194-8750-ffbfe516a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "objectness, pred_bbox_deltas = model.model.rpn.head(list(features.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b5abf-afb3-4b28-b79c-b2f3d5ec9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da368d5a-f6b8-4794-a15d-466d38f86fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dddc7a2-ca60-4bc8-849e-85ee8b50de1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867f024-2ffb-4ecd-9c16-e17e99265a2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# b[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d0022-2625-4d5f-824e-1f792902cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04909a-6719-42e5-8b89-db62249333dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetectModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb79fc9-a0f9-42ab-b26e-8fdfa2247361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a54072-e74c-477f-b0f2-e53f69d36ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    # accelerator=config.accelerator,\n",
    "    # devices=[config.device],\n",
    "    # callbacks=[\n",
    "    #     checkpoint_callback,\n",
    "    #     EarlyStopping(monitor=config.monitor_metric, patience=4, mode=config.monitor_mode),\n",
    "    #     LearningRateMonitor(logging_interval='epoch'),\n",
    "    # ],\n",
    ")\n",
    "trainer.fit(model=model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbbbeaf-d987-43cd-a8b5-1d3873766f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef2ee41-855f-4ac5-b46a-c521a646d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, targets in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f48a9-b22b-4569-911c-acc663a6b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "thv.ops.box_iou(\n",
    "    th.LongTensor([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\n",
    "    th.LongTensor([[1,2,3,4],[1,2,3,4]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710d08b-6b6b-4676-a9f4-f805303947bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_dist_avail_and_initialized():\n",
    "    if not th.distributed.is_available():\n",
    "        return False\n",
    "    if not th.distributed.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    with torch.inference_mode():\n",
    "        names = []\n",
    "        values = []\n",
    "        # sort the keys so that they are consistent across processes\n",
    "        for k in sorted(input_dict.keys()):\n",
    "            names.append(k)\n",
    "            values.append(input_dict[k])\n",
    "        values = torch.stack(values, dim=0)\n",
    "        dist.all_reduce(values)\n",
    "        if average:\n",
    "            values /= world_size\n",
    "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "    return reduced_dict\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, scaler=None):\n",
    "    model.train()\n",
    "    loss_values = []\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "    for i_batch, (images, targets) in enumerate(data_loader):\n",
    "        # print(f\"{i_batch=}\")\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [\n",
    "            {k: v.to(device) for k, v in t.items() if k!=\"ocr\"} \n",
    "            for t in targets\n",
    "        ]\n",
    "        # with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "        loss_value = losses_reduced.item()\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "        loss_values.append(loss_value)\n",
    "    return loss_values\n",
    "\n",
    "model = thv.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    # pretrained=True\n",
    "    # weights=\"DEFAULT\"\n",
    "    \n",
    ")\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params, lr=0.05, momentum=0.9, weight_decay=0.0005\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=3, gamma=0.1\n",
    ")\n",
    "train_loader = data.train_dataloader()\n",
    "for epoch in range(15):\n",
    "    losses = train_one_epoch(\n",
    "        model, \n",
    "        optimizer, \n",
    "        train_loader, \n",
    "        device=th.device('cpu'), \n",
    "        epoch=0\n",
    "    )\n",
    "    lr_scheduler.step()\n",
    "    print(th.mean(th.FloatTensor(losses)),\":\", losses)\n",
    "    # def eval_step(model, data_loader):\n",
    "    data_loader = data.train_dataloader()\n",
    "    metric = MeanAveragePrecision()\n",
    "    model.eval()\n",
    "    pred_shapes = []\n",
    "    for i_batch, (images, targets) in enumerate(data_loader):\n",
    "        with th.no_grad():\n",
    "            pred = model(images)\n",
    "            pred_shapes.append([p[\"boxes\"].shape for p in pred])\n",
    "            losses = model(images, targets)\n",
    "        # print(f\"losses={losses}\")\n",
    "        metric.update(\n",
    "            preds=pred,target=targets\n",
    "        )\n",
    "    print(f\"{pred_shapes=}\")\n",
    "    print(f\"metrics={metric.compute()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e83935-b1c8-469e-bf71-4f1cce7f3748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3037c-82da-490c-905a-34cd42d2c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = thv.transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14145578-a703-4977-ba98-721bb06280f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c2826-b02e-488d-8736-aea95206e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.ImageDraw as ImageDraw\n",
    "from matplotlib.patches import Polygon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b547f3d-ca22-40ef-b535-63b4391106ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb192c8-05df-4dfe-9e0a-25d29dd96886",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xmin,ymin),(xmin,ymax),(xmax,ymax),(xmax,ymin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52803f-536c-4d15-aaff-eab7e5a80f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw = ImageDraw.Draw(img)\n",
    "# draw.polygon(([(xmin,ymin),(xmin,ymax),(xmax,ymax),(xmax,ymin)],), fill=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37d534-1e50-495f-8864-efa56071c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx=16\n",
    "# idx = 72\n",
    "idx = 78\n",
    "xmin,xmax=[int(x) for x in sorted(pred[0][\"boxes\"][idx,[0,2]].tolist())]\n",
    "ymin,ymax=[int(x) for x in sorted(pred[0][\"boxes\"][idx,[1,3]].tolist())]\n",
    "\n",
    "pol = Polygon(\n",
    "    [\n",
    "        (xmin,ymin),\n",
    "        (xmin,ymax),\n",
    "        (xmax,ymax),\n",
    "        (xmax,ymin),\n",
    "        (xmin,ymin)\n",
    "    ],\n",
    "    ec='orangered',\n",
    "    fc='none'\n",
    ")\n",
    "plt.imshow(img)\n",
    "plt.gca().add_patch(pol)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c64804-9a88-4c8a-9470-5c4539cb8181",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc8ca0-5842-4e94-a3f0-4286b42d2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee79df-0ebc-4a67-b862-a69ef155195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ea7e1-30a3-48e3-a952-9b4fcd333f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "[p[\"boxes\"].shape for p in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae9660-213a-4605-9813-0cd706beb494",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)\n",
    "#(images,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a087374-7276-4e83-8dc3-3887c05cfd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in pred if x[\"scores\"].nelement()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63215ec1-27bb-4ce1-9bd2-f3d022e0a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "thv.models.detection.faster_rcnn.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adc2c5-5c77-476a-925d-c6805d6fdcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f66d8-4f87-4d53-8a29-324f41e2258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c6647-ed96-4cfd-89c0-e29cac6afd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thv.ops.box_iou?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e689b-9e9c-44e0-bfcf-9da5b7c3a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c4545-c35d-4932-9c2e-f3ad85941737",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234bec94-b4b4-4d2e-ace1-eeae79c14776",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bef6c0-2dc6-4aa9-8e92-21e2518e07ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504010e1-ccae-4c11-8c21-8392f84dd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thv.ops.box_iou(\n",
    "    [x for x in pred if x[\"scores\"].nelement()],\n",
    "    targets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d21c61f-435b-482f-9655-1c6ed10b6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75df9d-4db7-44d7-9efc-db39bfaa5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895004e-10a6-48dd-bf93-6277f4e4337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[\"boxes\"] = b[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a893f-fde5-449e-b2d8-b24fcd883b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c48bb-3d1e-4c93-bae1-57c065740968",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.stack(b[\"boxes\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0d55c-824b-4803-af6a-a2a3fbd7dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[\"boxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74576aa-1761-4f81-8415-088fe4ef099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "for bbox in b[\"label\"]:\n",
    "    targets.append(\n",
    "        {\n",
    "            \"boxes\": \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5cb42-5870-49cf-b729-1396b9ca3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "th2pil = thv.transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b50893-5a57-4787-96f0-a70450a5337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "th2pil(b[\"image\"][0].tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f6e5d-3467-4592-b9d5-4e4c2f40b18b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelb[\"image\"][:1], [{\"boxes\":th.stack(b[\"boxes\"], axis=1)}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb36fb-f1ff-492e-b268-836edb7f0876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
